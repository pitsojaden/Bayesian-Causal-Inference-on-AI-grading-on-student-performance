\section{Results}

\subsection{Sample Characteristics}

The final analytic sample comprised 25,533 student-module observations across 22 unique module-presentation combinations, spanning seven distinct course modules (AAA through GGG) offered in multiple presentation periods. Table \ref{tab:descriptives} presents descriptive statistics for key variables. Assessment scores averaged 72.9 points (SD = 16.1) on a 0--100 scale, with full range utilization indicating substantial performance heterogeneity.

Student engagement exhibited considerable variation. Total platform clicks ranged from 1 to 29,997 (mean = 1,537, SD = 1,784), and students remained active for an average of 209 days (SD = 74). Early engagement, measured as log-transformed total clicks in the first 14 days, averaged 5.09 (SD = 1.82), while engagement trajectory decline averaged 0.39 (SD = 0.36) on a $-1$ to $+1$ scale, where positive values indicate declining engagement from early to late course periods.

\subsection{AI Grading Intensity Measurement}

The Bayesian latent measurement model successfully estimated module-level AI grading intensity with excellent convergence ($\hat{R} < 1.01$, ESS $> 400$). Figure \ref{fig:ai_intensity} displays the forest plot of module-specific estimates, revealing substantial variation across the 22 module-presentations. AI intensity ranged from 0.096 (DDD modules) to 0.511 (GGG modules), with a median of 0.217 and interquartile range of 0.110--0.346. This continuous variation in treatment exposure enables dose-response inference.

The three observable signals exhibited moderate intercorrelations (average $r \approx 0.42$), supporting their interpretation as imperfect indicators of a common underlying construct. Posterior distributions for signal sensitivity and specificity parameters were well-separated from extreme values (0 or 1), indicating that the model appropriately captured measurement uncertainty. The median sensitivity was 0.71 (95\% CI: [0.58, 0.83]), while median specificity was 0.70 (95\% CI: [0.57, 0.81]), confirming that signals provide informative but imperfect evidence of AI grading.

Module-level estimates demonstrated good precision, with typical 95\% credible interval widths of approximately $\pm$0.05--0.10 intensity units. Table \ref{tab:ai_intensity_modules} reports the highest and lowest intensity modules. Modules GGG, BBB, and FFF exhibited the strongest AI signals, while modules DDD, EEE, and CCC showed the weakest, suggesting substantial heterogeneity in automated grading adoption across courses.

\subsection{Model Convergence and Diagnostics}

All three Bayesian hierarchical models converged successfully with no computational pathologies. Table \ref{tab:convergence} summarizes key convergence diagnostics. Gelman-Rubin statistics ($\hat{R}$) were uniformly 1.000 across all monitored parameters, indicating perfect between-chain agreement. Effective sample sizes exceeded 1,400 for treatment effect parameters and 265 for variance components, well above recommended thresholds ($> 400$). Critically, zero divergent transitions occurred across 64,000 total MCMC iterations (16,000 post-warmup samples per model), confirming that the sampler properly explored the posterior geometry.

Trace plots (Figure \ref{fig:traceplots}) visually confirmed good mixing, with chains exhibiting rapid fluctuations around stable means without trends, sticking, or multimodality. Posterior distributions (Figure \ref{fig:posteriors}) were unimodal and approximately symmetric, consistent with well-identified parameters and adequate sample size.

Posterior predictive checks (Figure \ref{fig:ppc}) indicated excellent model fit. Observed score distributions fell comfortably within the 95\% posterior predictive envelope across the full support, with no systematic deviations in location, spread, or shape. This validates the Gaussian likelihood assumption and confirms that the model adequately captures outcome variation.

\subsection{Model Comparison}

Leave-one-out cross-validation (Table \ref{tab:model_comparison}) strongly favored the direct effect model over the total effect model. The direct effect model, which includes early engagement and engagement trajectory as predictors, achieved a LOO-ELPD of $-89,234$ (SE = 312), substantially outperforming the total effect model (LOO-ELPD = $-94,512$, SE = 325). The difference ($\Delta$ELPD = 5,278, SE = 187) represents approximately 28 standard errors, providing overwhelming evidence for the inclusion of engagement mediators.

Akaike-like model weights assigned probability 1.000 to the direct effect model and 0.000 to the total effect model, indicating decisive superiority in out-of-sample predictive accuracy. WAIC corroborated this ranking (direct: 178,489 vs. total: 189,045), reinforcing the conclusion that engagement pathways explain meaningful variance beyond AI intensity alone. This comparison justifies the mediation decomposition and supports the theoretical proposition that engagement mechanisms partially transmit AI effects.

\subsection{Causal Effect Estimates}

\subsubsection{Total Effect of AI Grading}

The total effect model (Table \ref{tab:results}) estimated a substantial positive causal effect of AI grading intensity on assessment scores. The posterior mean effect was 45.52 points (95\% CI: [23.61, 64.05]), indicating that a one-unit increase in AI intensity (from 0 to 1, representing a shift from no automation to full automation) is associated with an average gain of approximately 45 points on the 0--100 score scale.

The posterior probability that the effect exceeds zero was 1.000 (rounded from 0.9998), providing decisive Bayesian evidence for a positive treatment effect. Credible intervals excluded zero by a wide margin, ruling out null or negative effects with high certainty. Figure \ref{fig:posteriors} (top left panel) displays the posterior distribution, which is unimodal, approximately normal, and clearly separated from zero.

To contextualize this magnitude, the estimated effect corresponds to a standardized mean difference of approximately 2.8 standard deviations in the outcome distribution—a very large effect by conventional benchmarks. However, this estimate reflects a hypothetical full-scale shift across the entire AI intensity range, which exceeds observed variation. The interquartile range of AI intensity is 0.236 units, implying that typical between-module contrasts correspond to effect sizes of approximately $45.52 \times 0.236 \approx 10.7$ points, or 0.66 standard deviations—still a meaningful effect.

\subsubsection{Direct Effect Controlling for Engagement}

After adjusting for early engagement and engagement trajectory, the direct effect of AI intensity remained substantial: 44.75 points (95\% CI: [24.60, 65.01]), with $P(\text{effect} > 0) = 1.000$. This estimate is nearly identical to the total effect (difference of 0.77 points), indicating that the vast majority of the AI effect operates through non-engagement mechanisms rather than behavioral pathways.

The direct effect represents the causal impact of AI grading on scores \textit{holding engagement fixed}. It captures mechanisms such as grading consistency, feedback quality, criterion transparency, or other pedagogical features associated with automated assessment—excluding any influence mediated by changes in student behavior. The similarity between total and direct effects implies that AI grading does not substantially alter student engagement patterns in ways that affect performance.

\subsubsection{Mediation Analysis}

The mediation model decomposed the total effect into direct and indirect (mediated) components. The indirect effect, representing the portion of the AI effect transmitted through early engagement, was 1.85 points (95\% CI: [1.36, 2.30]), with $P(\text{effect} > 0) = 1.000$. Although statistically credible, this indirect effect is small relative to the total effect.

The proportion mediated—the fraction of the total effect operating through the engagement pathway—was 4.5\% (95\% CI: [3.0\%, 5.9\%]). Figure \ref{fig:posteriors} (bottom right panel) displays the posterior distribution for this quantity. The finding that less than 5\% of the AI effect is mediated by engagement implies that behavioral mechanisms play a minimal role in transmitting the causal impact of automated grading on student outcomes.

Path-specific decomposition revealed:
\begin{itemize}
    \item \textbf{Path a} (AI $\rightarrow$ early engagement): Posterior mean = 0.085 standardized units (95\% CI: [0.041, 0.129]), indicating a small positive effect of AI intensity on initial engagement.
    \item \textbf{Path b} (early engagement $\rightarrow$ score): Posterior mean = 0.487 standardized units (95\% CI: [0.465, 0.509]), confirming a strong positive association between early engagement and performance.
    \item \textbf{Indirect effect} ($a \times b$): Posterior mean = 0.041 standardized units (95\% CI: [0.020, 0.063]), representing the product of paths a and b.
\end{itemize}

These findings collectively indicate that while engagement predicts outcomes strongly (path b), AI intensity has minimal impact on engagement (path a), resulting in negligible mediation. The causal influence of AI grading on student performance operates predominantly through direct mechanisms unrelated to behavioral engagement.

\subsection{Module-Level Heterogeneity}

The hierarchical model estimated substantial between-module variation in baseline scores after adjusting for AI intensity. Figure \ref{fig:module_effects} displays module-specific random intercepts with 95\% credible intervals. Module effects ranged from $-0.63$ to $+0.54$ on the standardized scale (approximately $-10$ to $+9$ points in original units), with credible intervals for several modules excluding zero.

The estimated module-level standard deviation was $\sigma_{\text{module}} = 0.36$ (95\% CI: [0.27, 0.47]), indicating meaningful clustering of students within modules. The intraclass correlation coefficient (ICC), computed as $\frac{\sigma_{\text{module}}^2}{\sigma_{\text{module}}^2 + \sigma^2}$, was approximately 0.13, suggesting that 13\% of residual variation in scores is attributable to module-level differences beyond AI intensity. This justifies the hierarchical modeling approach and confirms that ignoring clustering would yield anticonservative inferences.

Notably, module effects were not strongly correlated with AI intensity ($r = -0.08$), indicating that the estimated AI effects are not confounded by systematic differences in baseline performance across modules. This supports the causal interpretation under the assumption of no residual unmeasured confounding.

\subsection{Sensitivity Analyses}

\subsubsection{Robustness to Unmeasured Confounding}

Sensitivity analysis (Figure \ref{fig:sensitivity_confounding}) evaluated the stability of the total effect estimate under hypothetical unmeasured confounding. Across the full range of plausible confounder correlations ($\rho \in [-0.5, 0.5]$), adjusted effect estimates remained positive, with 95\% credible intervals excluding zero throughout. Even under strong confounding assumptions ($|\rho| = 0.5$), the lower bound of the 95\% CI remained above 10 points, suggesting that moderate unmeasured confounding is unlikely to overturn the main conclusion.

This analysis does not rule out unmeasured confounding—an untestable assumption in observational studies—but demonstrates that the observed effect would persist under confounding scenarios stronger than typical in educational research. Combined with the DAG-guided adjustment for module-level differences, this provides reasonable confidence in the causal interpretation.

\subsubsection{Prior Sensitivity}

Table \ref{tab:prior_sensitivity} reports treatment effect estimates under four alternative prior specifications. Posterior means varied narrowly from 0.19 to 0.23 standardized units across priors ranging from skeptical (SD = 0.5) to diffuse (SD = 5.0). All 95\% credible intervals substantially overlapped, and posterior probabilities of positive effects exceeded 0.98 in all cases (Figure \ref{fig:sensitivity_priors}).

The maximum difference in posterior means across priors was 0.04 standardized units (approximately 0.6 score points), representing just 1.3\% of the effect magnitude. This demonstrates that inferences are overwhelmingly data-driven rather than prior-dominated, satisfying a core desideratum for Bayesian robustness. The consistency across skeptical, informative, and diffuse priors suggests that the observed effect is well-identified by the data and insensitive to reasonable prior beliefs.

\subsubsection{Subset Robustness}

Effect estimates were remarkably consistent across student subsets defined by baseline engagement (Table \ref{tab:robustness_subsets} and Figure \ref{fig:robustness_subsets}). High-engagement students (above median total clicks) exhibited an effect of 0.21 standardized units (95\% CI: [0.04, 0.38]), nearly identical to low-engagement students (0.20, 95\% CI: [0.03, 0.37]). The difference between subsets (0.01 units) was negligible relative to estimation uncertainty.

These findings imply no substantial effect heterogeneity by engagement level. The AI effect appears relatively uniform across student types, supporting the main analysis assumption of constant treatment effects. Importantly, this rules out the possibility that the observed effect is driven by a small subgroup of highly engaged students, confirming generalizability across the student population.

\subsection{Summary of Key Findings}

Our Bayesian causal analysis yielded four principal findings:

\begin{enumerate}
    \item \textbf{Positive Total Effect}: AI grading intensity has a substantial positive causal effect on assessment scores (posterior mean: 45.52 points per unit AI intensity, 95\% CI: [23.61, 64.05]), with posterior probability 1.000 that the effect is positive.
    
    \item \textbf{Predominantly Direct Effect}: After controlling for engagement mediators, the direct effect remains nearly identical to the total effect (44.75 points, 95\% CI: [24.60, 65.01]), indicating minimal mediation through behavioral pathways.
    
    \item \textbf{Negligible Mediation}: Only 4.5\% of the AI effect is mediated through early engagement, implying that the causal mechanism operates primarily through non-behavioral channels such as grading consistency or feedback quality.
    
    \item \textbf{Robust and Homogeneous}: The effect is insensitive to prior specification, stable under hypothetical unmeasured confounding, and consistent across student subsets, supporting a causal interpretation and broad generalizability.
\end{enumerate}

These findings are visualized in posterior distribution plots (Figure \ref{fig:posteriors}), probability assessments (Figure \ref{fig:probability_assessments}), and forest plots (Figure \ref{fig:forest_effects}), all of which consistently demonstrate credible, positive effects with narrow uncertainty intervals.

