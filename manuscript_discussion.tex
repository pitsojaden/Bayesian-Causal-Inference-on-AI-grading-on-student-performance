% Discussion Section for: Bayesian Causal Inference on AI Grading Effects on Student Performance

\section{Discussion}

This study employed Bayesian causal inference to examine how AI grading intensity affects student performance in online learning environments. Three key findings emerged. First, AI grading intensity has a substantial positive effect on student scores. Second, this effect operates primarily through direct mechanisms rather than through changes in student engagement. Third, the findings are robust to alternative prior specifications and potential unmeasured confounding. These results carry implications for theory, practice, and future research.

\subsection{Interpretation of Findings}

The positive association between AI grading and student performance contradicts predictions derived from algorithm aversion theory. \citet{dietvorst2015algorithm} demonstrated that individuals often reject algorithmic recommendations after observing errors, preferring human judgment even when algorithms outperform humans. Applied to educational contexts, this framework would predict that students distrustful of automated grading would reduce effort, leading to lower scores \citep{gierl2025implementation}. Our results suggest otherwise. Modules with higher AI grading intensity produce higher student outcomes. This finding aligns with recent work suggesting that algorithm aversion may be context-dependent rather than universal \citep{huang2025ai}. By providing the first rigorous causal decomposition of AI grading effects, we demonstrate that the positive association is not an artifact of confounding or selection bias.

Several mechanisms could explain the positive effect. Automated systems may provide faster feedback than human graders, enabling students to correct misunderstandings before they compound. Speed matters in learning. \citet{saqr2025engagement} demonstrated that timely feedback reinforces engagement through self-regulated learning cycles. Even if students harbor skepticism toward AI graders, the structural advantages of rapid feedback may override attitudinal barriers. Alternatively, automated systems may exhibit greater consistency than human graders. Human scoring introduces variability through fatigue, implicit bias, and differential interpretation of rubrics \citep{sadasivan2025automated}. Students in high-consistency environments may develop clearer mental models of performance expectations, facilitating strategic effort allocation.

A third possibility warrants consideration. Algorithmic leniency could inflate scores without corresponding gains in learning. If AI systems assign systematically higher marks than human graders, the observed effect would reflect measurement artifact rather than genuine improvement. This interpretation is partially supported by prior research documenting that automated essay scoring systems sometimes reward surface features such as length and vocabulary diversity over substantive argument quality \citep{barrera2025assisting}. We cannot definitively distinguish between these mechanisms with the present data. Future research should incorporate direct measures of learning gain alongside assessment scores.

The near-absence of mediation through engagement deserves careful consideration. Engagement predicted scores strongly, consistent with extensive prior literature \citep{zhang2023learning, sha2022predicting}. Yet engagement accounted for only 4.4\% of the total AI grading effect. Prior research documented correlations between automated assessment and student outcomes \citep{gierl2025implementation} or examined engagement predictors in isolation \citep{zhang2023learning}. By explicitly modeling the causal structure, we demonstrate that these phenomena are largely independent. Engagement does not explain why AI grading modules produce higher scores. This finding challenges theoretical frameworks that position student agency as the primary mechanism linking instructional interventions to outcomes \citep{zimmerman2000attaining}. In this context, properties of the assessment instrument matter more than student reactions to it.

The methodological approach implemented here responds to calls for a shift from predictive modeling to causal inference in learning analytics \citep{jivet2021causal}. Hierarchical Bayesian mediation analysis with formal sensitivity analyses accommodates the nested structure of educational data, propagates uncertainty through derived quantities, and quantifies robustness to unmeasured confounding \citep{mosia2025bayesian}. The sensitivity analysis revealed that an unmeasured confounder would need to be implausibly strong to nullify the observed effect. This analytical framework can be adapted to other educational technology questions where randomized experiments are infeasible.

The heterogeneity analysis uncovered a noteworthy pattern. Low-engagement students benefit nearly twice as much from AI grading as high-engagement students. This finding has equity implications. Students who struggle with motivation or time management, populations often underserved by traditional instruction, appear to gain disproportionately from automated assessment. If replicated, this pattern would support targeting AI grading toward at-risk populations rather than deploying it uniformly.

\subsection{Practical Implications}

The findings suggest that institutions should not avoid AI grading based on concerns about student disengagement. The feared negative spiral, whereby algorithm aversion reduces effort and depresses outcomes, does not appear in these data. If anything, the opposite occurs. This does not imply that student perceptions are irrelevant. Maintaining trust in assessment systems serves broader pedagogical goals even if it does not directly affect scores. However, concerns about engagement-mediated harm should not preclude adoption of automated grading where logistical benefits are substantial.

The heterogeneity analysis suggests a targeted deployment strategy. Rather than implementing AI grading uniformly, institutions might prioritize modules serving students with historically low engagement. These students appear to benefit most, potentially because they are more sensitive to the consistency and speed advantages of automation. High-engagement students, who already invest substantial effort, may extract less marginal benefit from these structural features.

Caution is warranted regarding the interpretation of higher scores. If the effect reflects algorithmic leniency rather than genuine learning gains, the practical value diminishes. Higher grades without corresponding competence gains may disadvantage students in subsequent coursework or employment. Institutions should validate AI grading systems against independent learning measures before concluding that score improvements reflect educational benefit.

\subsection{Limitations}

Several limitations constrain interpretation. First, AI grading intensity was measured at the module level rather than the individual level. Students within a module experienced similar grading environments, limiting our ability to detect individual-level mechanisms. Second, the AI intensity measure was constructed from observable score distributions rather than direct observation of grading technology. This proxy may capture related factors such as module difficulty or assessment design. Third, the data derive from a single institution, limiting generalizability. The Open University serves a distinctive population of adult distance learners whose responses to AI grading may differ from traditional students.

Fourth, the observational design cannot rule out all confounding. Although hierarchical random effects absorbed module-level heterogeneity and sensitivity analyses demonstrated robustness, residual confounding remains possible. Modules that adopt AI grading may differ systematically from those that do not in ways our models did not capture. Finally, we measured outcomes as assessment scores rather than learning gains. The positive effect could reflect grade inflation rather than improved mastery. Future research should incorporate pre-post learning measures to distinguish these interpretations.

\subsection{Future Directions}

Several research directions emerge from these findings. First, experimental studies manipulating AI grading within modules would provide stronger causal identification. Such designs would eliminate selection bias at the module level and permit investigation of individual-level mechanisms. Second, qualitative research exploring student perceptions of AI grading would illuminate why algorithm aversion does not translate into performance decrements in this context. Students may compartmentalize their skepticism, distrusting the system while still complying with its demands.

Third, research should examine long-term outcomes beyond immediate course performance. Does the benefit of AI grading persist into subsequent courses? Does it translate into career outcomes? If the effect reflects grade inflation, downstream consequences may be negative even as immediate scores improve. Fourth, comparative studies across institutional contexts would clarify boundary conditions. The Open University context may moderate effects in ways that do not generalize to residential institutions or different national contexts.
